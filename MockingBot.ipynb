{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keysmusician/MockingBot/blob/main/MockingBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAgHxUgZ2qbF"
      },
      "source": [
        "#MockingBot\n",
        "This is my portfolio project for Holberton School.\n",
        "\n",
        "MockingBot is a generative model which aims to generate original audio similar to but not exactly the same as the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mHt7yy5LE7GP",
        "outputId": "0f630562-38c6-4233-bfba-db6f7b5f5259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow-io as it is not installed.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 5.8 kB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 47.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 438 kB 72.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 48.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 53.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Prepares dependencies.\n",
        "\n",
        "Note: If the runtime disconnects, reconnecting may not cause dependencies to \n",
        "load correctly. In that event, restart the runtime, then retry execution.\n",
        "'''\n",
        "# Install Tensorflow I/O so 24-bit WAV files can be opened. For some reason,\n",
        "# the preinstalled TensorFlow and Tensorflow I/O have to be removed for \n",
        "# libtensorflow_io.so to work.\n",
        "!pip uninstall tensorflow -yq # -yq: yes, quiet\n",
        "!pip uninstall tensorflow-io -yq\n",
        "!pip install tensorflow-gpu -q\n",
        "!pip install --no-deps tensorflow-io -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl3w_qpnGz19"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "This section prepares the various datasets I will be using.\n",
        "\n",
        "I currently have two datasets available:\n",
        "- Kicks: 856 24-bit ?-kHz stereo kick drum samples\n",
        "- Meows: 440 16-bit 8-kHz mono cat meows from 2 breeds in 3 situation categories (grooming, feeding, exploring). Source: https://zenodo.org/record/4008297#.YwC5xPHMJAd\n",
        "\n",
        "TODO:\n",
        "I need data from a wider range of sources. I currently have about 900 kick drum\n",
        "samples. I may want datasets of:\n",
        "- Cricket chirps\n",
        "- A bird call (how appropriate)\n",
        "- Katydid songs\n",
        "- Cow moos\n",
        "- Bullfrog croaks\n",
        "- Any short and simple animal call, etc.\n",
        "\n",
        "I think three sources should suffice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxrus9mqn88e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea43c943-5b2a-4034-cbde-ea8441ff655e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Datasets:\n",
            "Kicks\n",
            "Meows\n",
            "Output\n",
            ".ipynb_checkpoints\n",
            "Models\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Makes datasets available.\n",
        "'''\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "DATASETS_PATH = '/content/drive/Shareddrives/MockingBot/'\n",
        "\n",
        "# Show available datasets:\n",
        "print('Datasets:')\n",
        "\n",
        "for dataset_name in os.listdir(DATASETS_PATH):\n",
        "    print(dataset_name)\n",
        "\n",
        "\n",
        "# Uncomment the following if you'd like to upload a dataset manually.\n",
        "# You'll have to update the dataset path to point to your custom location:\n",
        "\n",
        "# from google.colab import files\n",
        "# files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgUvCV02qnNW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e4286401-47cd-480c-ab38-274ec8aeba26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The path to the dataset.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''\n",
        "Sets the project's dataset.\n",
        "'''\n",
        "# Choose the desired dataset here.\n",
        "# `DATASET_NAME` can be the name of any folder inside `DATASETS_PATH`:\n",
        "# NOTE: \"Kicks\" actually does not work because the WAV files can be saved in\n",
        "# various structures, not all of which are supported by TensorFlow. The files\n",
        "# in \"Kicks\" are not all in a consistent compatible structure.\n",
        "DATASET_NAME = 'Meows'\n",
        "\n",
        "DATASET_PATH = f'{DATASETS_PATH}{DATASET_NAME}/'\n",
        "'The path to the dataset.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alon1VfDeHaZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "41818baa-c98d-42f0-d91e-a440ae045de8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nConverts WAV files to be compatible with TensorFlow.\\n\\nThis cell is very incomplete.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "'''\n",
        "Converts WAV files to be compatible with TensorFlow.\n",
        "\n",
        "This cell is very incomplete.\n",
        "'''\n",
        "\n",
        "# file_count = len(training_filenames)\n",
        "# for i, filename in enumerate(training_filenames[:100]):\n",
        "#   print(f'{i}/{file_count} ({i / file_count * 100:.3}%)')\n",
        "#   try:\n",
        "#     test_audio_tensor = tfio.audio.decode_wav(\n",
        "#       input=tf.io.read_file(DATASET_PATH + filename),\n",
        "#       dtype=tf.int16\n",
        "#     )\n",
        "#     print(f'PASS: {filename}')\n",
        "#   except:\n",
        "#     try:\n",
        "#       test_audio_tensor = tfio.audio.decode_wav(\n",
        "#         input=tf.io.read_file(DATASET_PATH + filename),\n",
        "#         dtype=tf.int32\n",
        "#       )\n",
        "#       print(f'PASS: {filename}')\n",
        "#     except:\n",
        "#       print(f'FAIL:{filename}')\n",
        "#       \n",
        "# with wave.open(DATASET_PATH + 'SD_Kick_Jelly.wav', 'wb') as wav:\n",
        "#   wav.setnchannels(2)\n",
        "#   wav.setsampwidth(2)\n",
        "#   wav.setframerate(44100)\n",
        "#   # bit_depth = wav.getsampwidth() * 8\n",
        "#   # sample_rate = wav.getframerate()\n",
        "# \n",
        "# print(bit_depth, sample_rate)\n",
        "# \n",
        "# tfio.audio.decode_wav(\n",
        "#       input=tf.io.read_file(DATASET_PATH + 'SD_Kick_Jelly.wav'),\n",
        "#       dtype=tf.int16\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwSPAuy3rOGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a5d3b970-03e7-45ac-8bcc-874bae35a282"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9fa0bd7ddcda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_io\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_io'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "'''\n",
        "Loads and plots a random WAV file from the dataset.\n",
        "'''\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import wave\n",
        "\n",
        "\n",
        "filenames = [\n",
        "    filename for filename in os.listdir(DATASET_PATH)\n",
        "    if filename.endswith('.wav')\n",
        "]\n",
        "\n",
        "test_filename = random.choice(filenames)\n",
        "\n",
        "print('File name:', test_filename)\n",
        "\n",
        "test_file_path = DATASET_PATH + test_filename\n",
        "\n",
        "with wave.open(test_file_path) as wav:\n",
        "    bit_depth = wav.getsampwidth() * 8\n",
        "    sample_rate = wav.getframerate()\n",
        "\n",
        "print('Bit depth:', bit_depth)\n",
        "\n",
        "print('Sample rate:', sample_rate)\n",
        "\n",
        "test_audio_tensor = tfio.audio.decode_wav(\n",
        "    input=tf.io.read_file(test_file_path),\n",
        "    dtype=tf.int16 if bit_depth == 16 else tf.int32  # If this line errors, you may need to restart the runtime\n",
        ")\n",
        "\n",
        "print('Tensor dimensions:', test_audio_tensor.shape)\n",
        "\n",
        "plt.plot(test_audio_tensor)\n",
        "\n",
        "IPython.display.display(IPython.display.Audio(test_file_path, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00lkUCOXvock"
      },
      "source": [
        "#Input Pipeline\n",
        "The input pipeline for this project flows as follows:\n",
        "1. Construction. Audio tensors are constructed from each WAV file specified.\n",
        "2. Mono conversion. Audio tensors are converted to mono. Channel 1 (right?) is dropped if it exists. The resulting tensor has one axis of variable length.\n",
        "3. Amplitude normalization. Signals are amplitude normalized to fully utilize the amplitude space.\n",
        "4. Min-max scaling. Tensors are scaled between -1 and 1.\n",
        "5. Short-time Fourier Transform. Frequency information is extracted using STFT.\n",
        "\n",
        "If I want to simplify the problem, I can:\n",
        "- Pad tensors to equal length\n",
        "- Downsample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JYqH_obrz0B"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Builds and preprocesses the dataset.\n",
        "'''\n",
        "\n",
        "\n",
        "def load_wav(file_path):\n",
        "    '''\n",
        "    Loads a WAV file as a tensor.\n",
        "\n",
        "    Stereo files will be flattened to be mono by taking channel 0.\n",
        "\n",
        "    file_path: The path of a WAV file.\n",
        "\n",
        "    Returns: Variable length `tf.Tensor`.\n",
        "    '''\n",
        "    # TFIO decoding\n",
        "    # audio = tfio.audio.decode_wav(\n",
        "    #   input=tf.io.read_file(file_path),\n",
        "    #   dtype=tf.int16 if bit_depth == 16 else tf.int32\n",
        "    # )\n",
        "    #\n",
        "    # Flatten to mono if neccessary and remove the channel axis\n",
        "    # return audio[:, 0]\n",
        "\n",
        "    # TF decoding\n",
        "    audio, _ = tf.audio.decode_wav(\n",
        "        contents=tf.io.read_file(file_path),\n",
        "        desired_channels=1,\n",
        "        desired_samples=13_000\n",
        "    )\n",
        "\n",
        "    return tf.squeeze(audio)[2_000:]\n",
        "\n",
        "\n",
        "def normalize(audio_tensor):\n",
        "    '''\n",
        "    Normalizes an audio signal.\n",
        "\n",
        "    Scales an audio signal to entirely fill the range -1 to 1.\n",
        "\n",
        "    audio_tensor: A tensor of audio data.\n",
        "\n",
        "    Returns: `tf.float32` Normalized audio tensor.\n",
        "    '''\n",
        "    data_type_max = audio_tensor.dtype.max\n",
        "\n",
        "    tensor_max = tf.reduce_max(tf.abs(audio_tensor))\n",
        "\n",
        "    # Ensure `tensor_max` is non-zero to avoid arithmetic error\n",
        "    scaling_factor = tf.cast(data_type_max / tensor_max, tf.float32)\\\n",
        "        if tensor_max != 0 else 1.0\n",
        "\n",
        "    return tf.cast(audio_tensor, tf.float32) * scaling_factor / data_type_max\n",
        "\n",
        "\n",
        "def input_pipeline(file_path):\n",
        "    '''\n",
        "    Performs dataset processing.\n",
        "\n",
        "    file_path: A string tensor containing the name of a WAV file in the dataset.\n",
        "\n",
        "    Returns: A 2D tensor of audio features (see STFT).\n",
        "    '''\n",
        "    spectrogram_vector = tf.reshape(\n",
        "        tf.abs(\n",
        "            tf.signal.stft(\n",
        "                # TF decoding does normalization (TFIO does not):\n",
        "                # normalize(load_wav(file_path)), \n",
        "                load_wav(file_path),\n",
        "                frame_length=2048,\n",
        "                frame_step=50\n",
        "            )\n",
        "        ),\n",
        "        [-1]  # Flatten the spectrogram\n",
        "    )\n",
        "\n",
        "    max = tf.reduce_max(spectrogram_vector)\n",
        "\n",
        "    return spectrogram_vector / max\n",
        "\n",
        "\n",
        "# Try tf...Dataset.list_files() instead\n",
        "training_dataset = tf.data.Dataset.list_files(\n",
        "    file_pattern=DATASET_PATH + '*.wav',\n",
        "    shuffle=True,\n",
        "    seed=0\n",
        ").map(\n",
        "    map_func=input_pipeline,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ").filter(\n",
        "    lambda training_example:\n",
        "      (not tf.math.reduce_any(tf.experimental.numpy.isnan(training_example))) \n",
        "      and\n",
        "      (not tf.math.reduce_any(tf.experimental.numpy.isinf(training_example)))\n",
        ")\n",
        "\n",
        "# Test the dataset:\n",
        "for training_example in training_dataset:\n",
        "    # Confirm there are no NaN's or inf's in the dataset:\n",
        "    tf.debugging.assert_all_finite(training_example, str(training_example))\n",
        "\n",
        "    # Confirm the values are normalized\n",
        "    if not tf.experimental.numpy.isclose(\n",
        "          tf.reduce_max(training_example).numpy(), 1):\n",
        "      raise ValueError(f'Tensor is not normalized: {training_example}')\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ozkEcFRgxGi"
      },
      "outputs": [],
      "source": [
        "''' \n",
        "Creates a mock dataset of sin waves at various frequencies.\n",
        "'''\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def generate_mock_dataset():\n",
        "    training_example_count = 60_000\n",
        "\n",
        "    tau = np.pi * 2\n",
        "\n",
        "    time_steps = np.arange(0, 11_000)\n",
        "\n",
        "    sample_rate_Hz = 8_000\n",
        "\n",
        "    max_frequency_Hz = sample_rate_Hz / 2\n",
        "\n",
        "    min_frequency_Hz = 40\n",
        "\n",
        "    frequency_range_ratio = max_frequency_Hz / min_frequency_Hz\n",
        "\n",
        "    frequency_steps = 10\n",
        "\n",
        "    amplitude = 1\n",
        "\n",
        "    signals = []\n",
        "    \n",
        "    for frequency_step in range(frequency_steps):\n",
        "\n",
        "        signal_frequency_Hz = min_frequency_Hz * frequency_range_ratio ** (\n",
        "            frequency_step / frequency_steps)\n",
        "\n",
        "        signal = amplitude * np.sin(\n",
        "            tau * signal_frequency_Hz * time_steps / sample_rate_Hz, \n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        signals.append(signal)\n",
        "        \n",
        "    signals = np.stack(signals, 0)\n",
        "\n",
        "    signals = np.repeat(signals, 100, 0) # Generate multiple copies of the data so I can train for less epochs\n",
        "\n",
        "    def mock_input_pipeline(signal):\n",
        "        '''\n",
        "        Performs dataset processing.\n",
        "\n",
        "        signal: A tensor containing an audio signal.\n",
        "\n",
        "        Returns: A 2D tensor of audio features (see STFT).\n",
        "        '''\n",
        "        spectrogram_vector = tf.reshape(\n",
        "            tf.abs(\n",
        "                tf.signal.stft(\n",
        "                    signal,\n",
        "                    frame_length=2048,\n",
        "                    frame_step=50\n",
        "                )\n",
        "            ),\n",
        "            [-1]  # Flatten the spectrogram\n",
        "        )\n",
        "\n",
        "        max = tf.reduce_max(spectrogram_vector)\n",
        "\n",
        "        return spectrogram_vector / max\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices(signals).map(mock_input_pipeline)\n",
        "\n",
        "# training_dataset = generate_mock_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFQWljaic71p"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Demonstrates the short-time Fourier Transform.\n",
        "'''\n",
        "#@title STFT Demo { display-mode: \"both\" }\n",
        "#@markdown Toggle cell execution:\n",
        "run_cell = False #@param {type:\"boolean\"}\n",
        "\n",
        "if run_cell:\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "    tau = np.pi * 2\n",
        "    time_steps = tf.range(0, 11_000, dtype=tf.float32)\n",
        "    sample_rate = 8000\n",
        "    waveform = (\n",
        "        .8 * tf.sin(tau * 1_000 * time_steps / sample_rate) + \n",
        "        .3 * tf.sin(tau * 2_000 * time_steps / sample_rate)\n",
        "    )\n",
        "    spectrogram = tf.signal.stft(waveform, 2048, 50).numpy()\n",
        "    print('Spectrogram shape:', spectrogram.shape)\n",
        "    IPython.display.display(IPython.display.Audio(waveform, rate=sample_rate))\n",
        "\n",
        "    fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "    timescale = np.arange(waveform.shape[0])\n",
        "    axes[0].plot(timescale, waveform.numpy())\n",
        "    axes[0].set_title('Waveform')\n",
        "    axes[0].set_xlim([0, 1600])\n",
        "\n",
        "    log_spec = np.log(tf.abs(spectrogram).numpy().T + np.finfo(float).eps)\n",
        "    height = log_spec.shape[0]\n",
        "    width = log_spec.shape[1]\n",
        "    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "    Y = range(height)\n",
        "    axes[1].pcolormesh(X, Y, log_spec)\n",
        "    axes[1].set_title('Spectrogram')\n",
        "    plt.show()\n",
        "\n",
        "    # Audio:\n",
        "    reconstructed_signal = tf.signal.inverse_stft(\n",
        "        stfts=tf.cast(spectrogram, tf.complex64),\n",
        "        frame_length=2048,\n",
        "        frame_step=500,\n",
        "        window_fn=tf.signal.inverse_stft_window_fn(500),\n",
        "    )\n",
        "\n",
        "    IPython.display.display(IPython.display.Audio(reconstructed_signal, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3g5iZCxNkjd"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Defines the autoencoder architecture.\n",
        "''' \n",
        "import tensorflow.keras as keras\n",
        "\n",
        "\n",
        "def build_autoencoder(input_dims, hidden_layer_sizes, latent_dims, batch_size):\n",
        "    '''\n",
        "    Creates a variational autoencoder.\n",
        "\n",
        "    input_dims: An integer containing the dimensions of the model input.\n",
        "    hidden_layer_sizes: A list containing the number of nodes for each hidden\n",
        "        layer in the encoder, respectively.\n",
        "    latent_dims: An integer containing the dimensions of the latent space\n",
        "        representation.\n",
        "\n",
        "    Returns: (Encoder, Decoder, Autoencoder)\n",
        "        Encoder: The encoder model.\n",
        "        Decoder: The decoder model.\n",
        "        Autoencoder: The full autoencoder model.\n",
        "    '''\n",
        "    # Encoder architecture:\n",
        "    input_layer = keras.Input(\n",
        "        (input_dims,),\n",
        "        name='encoder_input'\n",
        "    ) \n",
        "\n",
        "    previous_layer = input_layer\n",
        "\n",
        "    for node_count in hidden_layer_sizes:\n",
        "        previous_layer = keras.layers.Dense(\n",
        "            node_count,\n",
        "            activation='relu'\n",
        "        )(previous_layer)\n",
        "\n",
        "    mean_layer = keras.layers.Dense(\n",
        "        latent_dims,\n",
        "        name='mean'\n",
        "    )(previous_layer)\n",
        "\n",
        "    log_variance_layer = keras.layers.Dense(\n",
        "        latent_dims,\n",
        "        name='log_variance'\n",
        "    )(previous_layer)\n",
        "\n",
        "    def normal_sample(inputs):\n",
        "        ''' Draws samples from a normal distribution. '''\n",
        "        mean, log_stddev = inputs\n",
        "\n",
        "        # Generate a batch of random samples\n",
        "        std_norm = tf.random.normal(\n",
        "            shape=(batch_size, latent_dims),\n",
        "            mean=0,\n",
        "            stddev=1\n",
        "        )  # KerasTensor\n",
        "\n",
        "        sample = mean + tf.exp(log_stddev / 2) * std_norm\n",
        "\n",
        "        return sample\n",
        "\n",
        "    sample_layer = keras.layers.Lambda(normal_sample)(\n",
        "        [mean_layer, log_variance_layer])\n",
        "\n",
        "    encoder_outputs = [sample_layer, mean_layer, log_variance_layer]\n",
        "\n",
        "    Encoder = keras.Model(input_layer, encoder_outputs, name='Encoder')\n",
        "\n",
        "    # Decoder architecture:\n",
        "    latent_space = keras.Input(\n",
        "        (latent_dims,),\n",
        "        name='decoder_input'\n",
        "    )\n",
        "\n",
        "    previous_layer = latent_space\n",
        "\n",
        "    for node_count in reversed(hidden_layer_sizes):\n",
        "        previous_layer = keras.layers.Dense(node_count, 'relu')(previous_layer)\n",
        "\n",
        "    decoder_layers = keras.layers.Dense(input_dims, 'sigmoid')(previous_layer)\n",
        "\n",
        "    Decoder = keras.Model(latent_space, decoder_layers, name='Decoder')\n",
        "\n",
        "\n",
        "    # Complete autoencoder\n",
        "    sample, mean, log_variance = Encoder(input_layer)\n",
        "\n",
        "    #sample = keras.backend.print_tensor(sample, '\\nsample:')\n",
        "    \n",
        "    reconstruction = Decoder(sample)\n",
        "    \n",
        "    Autoencoder = keras.Model(input_layer, reconstruction, name='autoencoder')\n",
        "\n",
        "    def VAE_loss(inputs, reconstructions, log_variance_layer, mean_layer):\n",
        "        ''' Custom loss function including a KL divergence term. '''\n",
        "        '''\n",
        "        reconstruction_loss = keras.losses.binary_crossentropy(\n",
        "            inputs, reconstructions) * input_dims\n",
        "\n",
        "        KL_loss = 1 + log_variance_layer - keras.backend.square(mean_layer) \\\n",
        "            - keras.backend.exp(log_variance_layer)\n",
        "\n",
        "        KL_loss = keras.backend.sum(KL_loss, axis=-1) * -0.5\n",
        "\n",
        "        total_loss = keras.backend.mean(reconstruction_loss + KL_loss)\n",
        "\n",
        "        return total_loss\n",
        "        '''\n",
        "        # log_variance_layer = keras.backend.print_tensor(\n",
        "        #    log_variance_layer, \"\\nLog variance:\")\n",
        "        \n",
        "        # mean_layer = keras.backend.print_tensor(mean_layer, \"Mean:\")\n",
        "\n",
        "        reconstruction_loss = keras.losses.binary_crossentropy(\n",
        "            inputs, reconstructions) * input_dims\n",
        "\n",
        "        # reconstruction_loss = tf.reduce_mean(\n",
        "        #     keras.backend.square(inputs - reconstructions))\n",
        "        \n",
        "        kl_loss = -0.5 * tf.reduce_mean(\n",
        "            1 + log_variance_layer - tf.square(mean_layer) - \n",
        "            tf.exp(log_variance_layer)\n",
        "        )\n",
        " \n",
        "        return keras.backend.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "    Autoencoder.add_loss(\n",
        "        VAE_loss(input_layer, reconstruction, log_variance, mean))\n",
        "\n",
        "    return (Encoder, Decoder, Autoencoder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYe5Uuv4dN67"
      },
      "outputs": [],
      "source": [
        "'''  \n",
        "Trains the model.\n",
        "'''\n",
        "batch_size = 44\n",
        "\n",
        "input_shape = training_dataset.element_spec.shape[0]\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "encoder, decoder, autoencoder = build_autoencoder(\n",
        "    input_shape, [256], 3, batch_size)\n",
        "\n",
        "autoencoder.load_weights(DATASETS_PATH + 'Models/' + 'VAE_256-3/' + f'VAE_1.h5')\n",
        "\n",
        "autoencoder.compile(\n",
        "    optimizer=keras.optimizers.Adam(\n",
        "        learning_rate=0.02\n",
        "    ), \n",
        "    loss=None\n",
        ")\n",
        "\n",
        "training_history = autoencoder.fit(\n",
        "    training_dataset.batch(batch_size), \n",
        "    epochs=40,\n",
        "    shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4Sx581koU8e"
      },
      "outputs": [],
      "source": [
        "'''  \n",
        "Generates and presents a sample from the autoencoder.\n",
        "'''\n",
        "WRITE_WAVE_FILE = False\n",
        "\n",
        "sample = list(np.random.normal(0, 1, 3)) # Random sample\n",
        "\n",
        "# sample = [-4.8, -12, -23.9]\n",
        "\n",
        "prediction = decoder.predict([sample])\n",
        "\n",
        "reconstructed_spectrogram = tf.reshape(prediction, (180, -1))\n",
        "\n",
        "reconstructed_signal = tf.signal.inverse_stft(\n",
        "    stfts=tf.cast(reconstructed_spectrogram, tf.complex64),\n",
        "    frame_length=2048,\n",
        "    frame_step=50,\n",
        "    window_fn=tf.signal.inverse_stft_window_fn(50),\n",
        ")\n",
        "\n",
        "reconstructed_spectrogram = reconstructed_spectrogram.numpy()\n",
        "\n",
        "\n",
        "# Present the reconstructed output:\n",
        "figure, axes = plt.subplots(2, figsize=(12, 8))\n",
        "\n",
        "axes[0].plot(reconstructed_signal)\n",
        "\n",
        "log_spec = np.log(reconstructed_spectrogram.T + np.finfo(float).eps)\n",
        "\n",
        "height, width = log_spec.shape\n",
        "\n",
        "X = np.linspace(0, np.size(reconstructed_spectrogram), num=width, dtype=int)\n",
        "\n",
        "Y = range(height)\n",
        "\n",
        "axes[1].pcolormesh(X, Y, log_spec)\n",
        "\n",
        "axes[1].set_title('Spectrogram')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "IPython.display.display(\n",
        "    IPython.display.Audio(reconstructed_signal, rate=sample_rate))\n",
        "\n",
        "# Save the WAV file:\n",
        "if WRITE_WAVE_FILE:\n",
        "    OUTPUT_DIRECTORY = DATASETS_PATH + 'Output/'\n",
        "    \n",
        "    file_count = len(\n",
        "        [f for f in os.listdir(OUTPUT_DIRECTORY) if f.endswith('.wav')])\n",
        "    \n",
        "    with wave.open(\n",
        "          OUTPUT_DIRECTORY + f'MockingBot_{file_count}.wav', 'wb') as wav:\n",
        "        wav.setparams(\n",
        "            (\n",
        "                1, # Channel count\n",
        "                2, # Sample width in bytes\n",
        "                sample_rate, # Sample rate\n",
        "                11_000,   # Sample count \n",
        "                'NONE',      # Compression type (must be None)\n",
        "                'not compressed' # Compression name\n",
        "            )\n",
        "        )\n",
        "        wav.writeframes(reconstructed_signal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNnJEVh854gn"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Saves the VAE parameters.\n",
        "'''\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "\n",
        "SAVE_MODEL_WEIGHTS = True\n",
        "\n",
        "if SAVE_MODEL_WEIGHTS:\n",
        "  \n",
        "    timestamp = pytz.utc.localize(datetime.now()).strftime('%m-%d-%y_%H-%M-%S')\n",
        "    autoencoder.save_weights(DATASETS_PATH + 'Models/' + f'VAE_{timestamp}.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh11WU8sOLKw"
      },
      "source": [
        "I haven't been able to reproduce results similar to the MNIST project using a VAE yet. What I can try:\n",
        "- Use same architecture as the MNIST project, including the number of input dimensions\n",
        "- Create a simpler dataset to see what archictecture is required to get good results on various types of input (how architecture needs or does not need to scale with input length, how many latent dimensions are required, whether a CNN would work better)\n",
        "\n",
        "I have determined that a VAE isn't going to give me the \"creativity\" I'm looking for, as it's optimizing for recreating the training examples. Consequently, I'm going to try a GAN instead."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MockingBot",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}